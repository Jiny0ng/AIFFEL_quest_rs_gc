{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6VwCfMsQNAw"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# 1. 라이브러리 불러오기\n",
        "# =========================================\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# =========================================\n",
        "# 2. 텍스트 전처리 함수 정의\n",
        "# =========================================\n",
        "def clean_korean_text(text):\n",
        "    \"\"\"\n",
        "    한국어 텍스트 전처리 함수\n",
        "    - 한글, 숫자, 공백, 문장부호(.,?! 제외)는 모두 제거\n",
        "    - 반복되는 'ㅋ', 'ㅎ'은 각각 'ㅋㅋ', 'ㅎㅎ'로 통일\n",
        "    - 같은 글자가 3번 이상 반복되면 2번으로 축소\n",
        "    - 다중 공백 제거\n",
        "    \"\"\"\n",
        "    if pd.isnull(text):  # NaN 값 방지\n",
        "        return \"\"\n",
        "    text = re.sub(r\"[^가-힣0-9\\s.,?!]\", \" \", text)   # 허용되지 않는 문자 제거\n",
        "    text = re.sub(r\"(ㅋ)\\1+\", \"ㅋㅋ\", text)          # ㅋㅋㅋ -> ㅋㅋ\n",
        "    text = re.sub(r\"(ㅎ)\\1+\", \"ㅎㅎ\", text)          # ㅎㅎㅎ -> ㅎㅎ\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)      # aaa -> aa\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()        # 다중 공백 제거\n",
        "    return text\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 3. 데이터 불러오기 및 전처리\n",
        "# =========================================\n",
        "# Train 데이터 로드\n",
        "train = pd.read_csv(\"merged_train.csv\")\n",
        "train[\"conversation\"] = train[\"conversation\"].astype(str).apply(clean_korean_text)\n",
        "\n",
        "# Test 데이터 로드\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "test[\"text\"] = test[\"text\"].astype(str).apply(clean_korean_text)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 4. 학습/검증 데이터 분리 (Stratified Split)\n",
        "# =========================================\n",
        "train_df, valid_df = train_test_split(\n",
        "    train,\n",
        "    test_size=0.2,             # 80:20 비율\n",
        "    random_state=42,           # 재현성 고정\n",
        "    stratify=train[\"class\"]    # 클래스 비율 유지\n",
        ")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 5. 토크나이저 정의\n",
        "# =========================================\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 6. 데이터셋 인코딩 함수\n",
        "# =========================================\n",
        "def encode_dataset(texts, labels, tokenizer, max_len=256):\n",
        "    \"\"\"\n",
        "    텍스트와 라벨을 토크나이저로 변환\n",
        "    - padding: max_length까지 패딩\n",
        "    - truncation: 길이 초과 시 자르기\n",
        "    - return_tensors: PyTorch 텐서 반환\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    encodings[\"labels\"] = torch.tensor(labels.tolist())\n",
        "    return encodings\n",
        "\n",
        "\n",
        "# Train/Valid 데이터 인코딩\n",
        "train_encodings = encode_dataset(train_df[\"conversation\"], train_df[\"class\"], tokenizer)\n",
        "valid_encodings = encode_dataset(valid_df[\"conversation\"], valid_df[\"class\"], tokenizer)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 7. Dataset 클래스 정의\n",
        "# =========================================\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 8. DataLoader 정의\n",
        "# =========================================\n",
        "train_dataset = ConversationDataset(train_encodings)\n",
        "valid_dataset = ConversationDataset(valid_encodings)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 9. 모델 정의 (BERT 분류기)\n",
        "# =========================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=5   # 클래스 개수\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 10. Optimizer & Loss 정의\n",
        "# =========================================\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,              # 학습률\n",
        "    betas=(0.9, 0.999),   # 1차, 2차 모멘트\n",
        "    eps=1e-8,             # 수치 안정성\n",
        "    weight_decay=0.01     # 가중치 감쇠 (정규화 효과)\n",
        ")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 11. 학습 루프\n",
        "# =========================================\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ----------- Train Loop -----------\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    all_preds, all_labels = [], []   # F1 점수 계산용\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 통계 계산\n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_acc = total_correct / len(train_dataset)\n",
        "    avg_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}, Train Acc: {avg_acc:.4f}, Train F1: {avg_f1:.4f}\")\n",
        "\n",
        "    # ----------- Validation Loop -----------\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(valid_loader)\n",
        "    avg_val_acc = val_correct / len(valid_dataset)\n",
        "    val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 12. 학습 완료 후 모델 저장\n",
        "# =========================================\n",
        "save_path = \"./saved_mbert_model\"\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 13. 저장된 모델 불러오기\n",
        "# =========================================\n",
        "save_path = \"./saved_mbert_model\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(save_path).to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(save_path)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 14. 예측 함수 정의\n",
        "# =========================================\n",
        "def predict(texts, model, tokenizer, max_len=256):\n",
        "    \"\"\"\n",
        "    입력 텍스트 리스트에 대해 예측 수행\n",
        "    - texts: pandas Series or list\n",
        "    - 반환값: 예측된 클래스 번호 배열\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "    return preds\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# 15. Test 데이터 예측 후 제출 파일 생성\n",
        "# =========================================\n",
        "preds = predict(test[\"text\"], model, tokenizer)\n",
        "\n",
        "# 샘플 제출 파일 불러오기\n",
        "submission = pd.read_csv(\"submission.csv\")\n",
        "\n",
        "# 예측 라벨 덮어쓰기\n",
        "submission[\"class\"] = preds\n",
        "\n",
        "# 저장\n",
        "submission.to_csv(\"mbert.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"✅ mbert.csv 저장 완료\")"
      ]
    }
  ]
}