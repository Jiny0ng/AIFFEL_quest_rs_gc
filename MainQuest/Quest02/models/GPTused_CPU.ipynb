{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026be00b-327b-4249-986c-137be526766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DKTC 5-클래스 분류 (GPT 기반, KoGPT2) - 고정 경로/학습 안정 버전 (CPU 강제)\n",
    "# - 학습: $HOME/work/DL thon/data/train.csv\n",
    "# - 예측: $HOME/work/DL thon/data/test.csv\n",
    "# - 제출 템플릿(읽기 전용): $HOME/work/DL thon/data/train.csv \n",
    "# - 결과 저장: $HOME/work/DL thon/data/GPT_submission.csv\n",
    "# =========================\n",
    "\n",
    "# 0) 필수 라이브러리 설치(없으면 설치)\n",
    "import sys, subprocess, importlib, os\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "for p in [\"transformers\", \"datasets\", \"accelerate\", \"scikit-learn\", \"pandas\", \"numpy\", \"torch\"]:\n",
    "    ensure(p)\n",
    "\n",
    "# 1) 임포트\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# EarlyStoppingCallback은 구버전에 없을 수 있음(있을 때만 사용)\n",
    "try:\n",
    "    from transformers import EarlyStoppingCallback\n",
    "    HAS_EARLY_STOP = True\n",
    "except Exception:\n",
    "    EarlyStoppingCallback = None\n",
    "    HAS_EARLY_STOP = False\n",
    "\n",
    "# 2) 고정 경로\n",
    "TRAIN_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/train.csv\"\n",
    "TEST_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/test.csv\"\n",
    "SUBMISSION_TEMPLATE_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/train.csv\"     # 읽기 전용 템플릿\n",
    "SUBMISSION_SAVE_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/GPT_submission.csv\"  # 새 파일로 저장\n",
    "\n",
    "# 3) 기본 설정\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "MODEL_NAME = \"skt/kogpt2-base-v2\"  # 한국어 GPT-2 (Decoder-only, GPT 계열)\n",
    "NUM_LABELS = 5\n",
    "NUM_FOLDS  = 5          # 데이터가 적으면 3\n",
    "EPOCHS     = 3\n",
    "BATCH_SIZE = 8\n",
    "LR         = 2e-5\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# 4) CSV 안전 읽기(인코딩 자동 감지)\n",
    "def read_csv_smart(path: str) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\", \"ISO-8859-1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "# 5) 라벨 정규화 설정\n",
    "CANONICAL_LABELS = [\n",
    "    \"협박\",\n",
    "    \"갈취\",\n",
    "    \"직장 내 괴롭힘\",\n",
    "    \"기타 괴롭힘\",\n",
    "    \"일상 대화\",\n",
    "]\n",
    "CODE_MAP = {\n",
    "    \"협박\": \"00\",\n",
    "    \"갈취\": \"01\",\n",
    "    \"직장 내 괴롭힘\": \"02\",\n",
    "    \"기타 괴롭힘\": \"03\",\n",
    "    \"일상 대화\": \"04\",\n",
    "}\n",
    "ALIASES = {\n",
    "    \"협박\": {\"협박\", \"협박대화\", \"협박 대화\"},\n",
    "    \"갈취\": {\"갈취\", \"갈취대화\", \"갈취 대화\"},\n",
    "    \"직장 내 괴롭힘\": {\n",
    "        \"직장내괴롭힘\", \"직장 내 괴롭힘\", \"직장 내괴롭힘\", \"직장내 괴롭힘\",\n",
    "        \"직장 내 괴롭힘대화\", \"직장 내 괴롭힘 대화\", \"직장내괴롭힘대화\"\n",
    "    },\n",
    "    \"기타 괴롭힘\": {\n",
    "        \"기타 괴롭힘\", \"기타괴롭힘\", \"기타-괴롭힘\",\n",
    "        \"기타 괴롭힘대화\", \"기타 괴롭힘 대화\"\n",
    "    },\n",
    "    \"일상 대화\": {\"일상 대화\", \"일상대화\", \"일반\", \"일반 대화\", \"일반대화\"},\n",
    "}\n",
    "def normalize_label_name(x: str) -> str:\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"(대화|유형|클래스|라벨)$\", \"\", s).strip()  # 접미사 제거\n",
    "    s_cmp = re.sub(r\"[\\s\\-\\_]\", \"\", s)\n",
    "    for canon, variants in ALIASES.items():\n",
    "        if s in variants or s_cmp in {re.sub(r'[\\s\\-\\_]', '', v) for v in variants}:\n",
    "            return canon\n",
    "    if \"협박\" in s: return \"협박\"\n",
    "    if \"갈취\" in s: return \"갈취\"\n",
    "    if (\"직장\" in s) and (\"괴롭힘\" in s): return \"직장 내 괴롭힘\"\n",
    "    if \"괴롭힘\" in s: return \"기타 괴롭힘\"\n",
    "    if (\"일상\" in s) or (\"일반\" in s): return \"일상 대화\"\n",
    "    if s in CANONICAL_LABELS: return s\n",
    "    return s\n",
    "\n",
    "# 6) 컬럼 자동 탐지\n",
    "def infer_text_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"text\", \"sentence\", \"utterance\", \"dialogue\", \"dialog\", \"conversation\",\n",
    "                  \"content\", \"data\", \"message\", \"문장\", \"대화\", \"텍스트\", \"내용\"]\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            sample = df[c].dropna().astype(str).head(20).tolist()\n",
    "            if any(len(str(t).strip()) > 0 for t in sample):\n",
    "                if c.lower() in candidates or c in candidates:\n",
    "                    return c\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if not obj_cols:\n",
    "        raise ValueError(\"텍스트 컬럼(object dtype)을 찾을 수 없습니다.\")\n",
    "    best_col = max(obj_cols, key=lambda c: df[c].dropna().astype(str).str.len().mean())\n",
    "    return best_col\n",
    "\n",
    "def infer_label_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"label\", \"labels\", \"class\", \"category\", \"target\", \"y\", \"라벨\", \"클래스\", \"카테고리\"]\n",
    "    for c in df.columns:\n",
    "        if c.lower() in candidates or c in candidates:\n",
    "            return c\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    for c in obj_cols:\n",
    "        uniq = set(df[c].dropna().astype(str).unique())\n",
    "        if any(normalize_label_name(u) in CANONICAL_LABELS for u in uniq):\n",
    "            return c\n",
    "    raise ValueError(\"라벨 컬럼을 자동 탐지할 수 없습니다. label/class 등으로 지정해 주세요.\")\n",
    "\n",
    "# 7) 데이터 로드 (원본 불변)\n",
    "train_df = read_csv_smart(TRAIN_PATH)\n",
    "test_df  = read_csv_smart(TEST_PATH)\n",
    "sub_tmpl = read_csv_smart(SUBMISSION_TEMPLATE_PATH)  # train.csv를 템플릿으로 사용\n",
    "\n",
    "# 8) 텍스트/라벨 컬럼 파악 및 정규화\n",
    "TEXT_COL = infer_text_column(train_df)\n",
    "LABEL_COL = infer_label_column(train_df)\n",
    "\n",
    "y_raw = train_df[LABEL_COL].astype(str).apply(normalize_label_name)\n",
    "unknown = sorted(set(y_raw.unique()) - set(CANONICAL_LABELS))\n",
    "if unknown:\n",
    "    raise ValueError(f\"다음 라벨 매핑 불가: {unknown} → normalize_label_name()에 패턴을 추가하세요.\")\n",
    "\n",
    "name2id = {name: i for i, name in enumerate(CANONICAL_LABELS)}\n",
    "id2name = {i: name for name, i in name2id.items()}\n",
    "y = y_raw.map(name2id)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "train_texts = train_df[TEXT_COL].astype(str).apply(clean_text).tolist()\n",
    "test_texts  = test_df[infer_text_column(test_df)].astype(str).apply(clean_text).tolist()\n",
    "\n",
    "# 8-1) FOLD 안전 설정(클래스 최소 개수 < 폴드수인 경우 자동 조정)\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "min_class_count = int(class_counts.min())\n",
    "if NUM_FOLDS > min_class_count:\n",
    "    old = NUM_FOLDS\n",
    "    NUM_FOLDS = max(2, min_class_count)\n",
    "    print(f\"[안내] 클래스 최소 개수({min_class_count})보다 폴드 수({old})가 큽니다 → {NUM_FOLDS}로 조정\")\n",
    "\n",
    "# 9) 토크나이저/모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    id2label={i: id2name[i] for i in range(NUM_LABELS)},\n",
    "    label2id={id2name[i]: i for i in range(NUM_LABELS)},\n",
    ")\n",
    "\n",
    "def tokenize_batch(texts: List[str]):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "# 10) 데이터셋\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "# 클래스 가중치 직접 계산(특정 fold에서 일부 클래스가 없어도 안전)\n",
    "def make_class_weight(labels: List[int], num_labels: int) -> torch.Tensor:\n",
    "    labels = np.array(labels, dtype=int)\n",
    "    N = len(labels); K = num_labels\n",
    "    counts = np.bincount(labels, minlength=K)\n",
    "    weights = np.zeros(K, dtype=np.float32)\n",
    "    for c in range(K):\n",
    "        weights[c] = N / (K * counts[c]) if counts[c] > 0 else 0.0\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# 11) Trainer 서브클래스(가중치 적용) - **kwargs로 추가 인자 흡수\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  # device 이동은 compute_loss에서\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 12) 평가 지표\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro}\n",
    "\n",
    "# == TrainingArguments를 버전에 맞춰 항상 일치되게 구성 + CPU 강제 ==\n",
    "def build_training_args(output_dir: str, fold_seed: int) -> TrainingArguments:\n",
    "    params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
    "    desired = dict(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        # 평가/저장 전략을 'epoch'로 통일 (구버전 키(eval_strategy)도 같이 넣음)\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,   # 미지원이면 아래에서 False 처리\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        seed=fold_seed,\n",
    "        dataloader_num_workers=2,\n",
    "        fp16=False,                    # 안정성 위해 혼합정밀 끔\n",
    "        gradient_accumulation_steps=1,\n",
    "        report_to=[],\n",
    "        no_cuda=True,                  # CPU 강제(구버전)\n",
    "        use_cpu=True,                  # CPU 강제(신버전)\n",
    "    )\n",
    "    kw = {k: v for k, v in desired.items() if k in params}\n",
    "    eval_key = \"evaluation_strategy\" if \"evaluation_strategy\" in params else (\"eval_strategy\" if \"eval_strategy\" in params else None)\n",
    "    save_ok = \"save_strategy\" in params\n",
    "    if \"load_best_model_at_end\" in kw and kw[\"load_best_model_at_end\"]:\n",
    "        if (eval_key is None) or (not save_ok):\n",
    "            kw[\"load_best_model_at_end\"] = False\n",
    "        else:\n",
    "            if eval_key not in kw:\n",
    "                kw[eval_key] = kw.get(\"save_strategy\", \"epoch\")\n",
    "            else:\n",
    "                if kw[eval_key] != kw.get(\"save_strategy\", None):\n",
    "                    kw[eval_key] = kw.get(\"save_strategy\", \"epoch\")\n",
    "    return TrainingArguments(**kw)\n",
    "\n",
    "# 13) Stratified K-Fold 분할 + 학습/예측\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "test_logits_accum = None\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train_texts, y), start=1):\n",
    "    print(f\"\\n===== FOLD {fold}/{NUM_FOLDS} =====\")\n",
    "    trn_texts = [train_texts[i] for i in trn_idx]\n",
    "    val_texts = [train_texts[i] for i in val_idx]\n",
    "    trn_labels = pd.Series(y).iloc[trn_idx].tolist()\n",
    "    val_labels = pd.Series(y).iloc[val_idx].tolist()\n",
    "\n",
    "    trn_enc = tokenize_batch(trn_texts)\n",
    "    val_enc = tokenize_batch(val_texts)\n",
    "    tst_enc = tokenize_batch(test_texts)  # 고정\n",
    "\n",
    "    train_ds = SimpleDataset(trn_enc, trn_labels)\n",
    "    val_ds   = SimpleDataset(val_enc, val_labels)\n",
    "    test_ds  = SimpleDataset(tst_enc, labels=None)\n",
    "\n",
    "    cls_weights = make_class_weight(trn_labels, NUM_LABELS)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # (중요) tokenizer 변경 가능성에 대비해 임베딩 크기 동기화\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # 사전 검증: 토큰 id 범위 & 라벨 범위 확인\n",
    "    with torch.no_grad():\n",
    "        sample_enc = tokenize_batch(trn_texts[:64])  # 작은 배치 샘플\n",
    "        max_id = int(np.max([np.max(ids) for ids in sample_enc[\"input_ids\"]]))\n",
    "        emb_n = model.get_input_embeddings().num_embeddings\n",
    "        assert max_id < emb_n, f\"토큰 id({max_id})가 임베딩 크기({emb_n})를 초과합니다.\"\n",
    "        assert all(0 <= int(lb) < NUM_LABELS for lb in trn_labels), \"라벨이 0~4 범위를 벗어났습니다.\"\n",
    "\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "    out_dir = f\"./gpt2_dktc_fold{fold}\"\n",
    "    args = build_training_args(out_dir, SEED + fold)\n",
    "\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] if HAS_EARLY_STOP else []\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=cls_weights,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 검증 성능 리포트(가능하면)\n",
    "    try:\n",
    "        val_pred_logits = trainer.predict(val_ds).predictions\n",
    "        val_preds = np.argmax(val_pred_logits, axis=-1)\n",
    "        print(classification_report(\n",
    "            val_labels, val_preds,\n",
    "            target_names=[id2name[i] for i in range(NUM_LABELS)],\n",
    "            digits=4\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"[경고] classification_report 출력 중 예외: {e}\")\n",
    "\n",
    "    # 테스트 로짓 앙상블(평균)\n",
    "    test_pred_logits = trainer.predict(test_ds).predictions  # [N_test, num_labels]\n",
    "    if test_logits_accum is None:\n",
    "        test_logits_accum = test_pred_logits\n",
    "    else:\n",
    "        test_logits_accum += test_pred_logits\n",
    "\n",
    "# 14) 테스트 최종 예측\n",
    "test_logits_mean = test_logits_accum / NUM_FOLDS\n",
    "test_pred_ids = np.argmax(test_logits_mean, axis=-1)  # 0~4\n",
    "id2name_local = {i: n for i, n in enumerate(CANONICAL_LABELS)}\n",
    "test_pred_names = [id2name_local[i] for i in test_pred_ids]\n",
    "test_pred_codes = [CODE_MAP[name] for name in test_pred_names]\n",
    "\n",
    "# 15) 제출 파일 생성(원본 템플릿은 읽기 전용, 새 파일에 저장)\n",
    "sub_df = sub_tmpl.copy()\n",
    "candidate_cols = [\"label\", \"class\", \"target\", \"pred\", \"prediction\"]\n",
    "label_col_to_use = None\n",
    "for c in sub_df.columns:\n",
    "    if c.lower() in candidate_cols:\n",
    "        label_col_to_use = c\n",
    "        break\n",
    "if label_col_to_use is None:\n",
    "    label_col_to_use = \"label\"\n",
    "    if \"label\" not in sub_df.columns:\n",
    "        sub_df[\"label\"] = None\n",
    "\n",
    "# 템플릿과 test 길이 불일치 시 test 기준으로 생성\n",
    "if len(sub_df) != len(test_pred_codes):\n",
    "    print(f\"[안내] 템플릿 행수({len(sub_df)}) != test 샘플 수({len(test_pred_codes)}). test 기준 새 프레임 생성.\")\n",
    "    sub_df = pd.DataFrame({label_col_to_use: test_pred_codes})\n",
    "else:\n",
    "    sub_df[label_col_to_use] = test_pred_codes\n",
    "\n",
    "# 저장(원본 파일 미변경)\n",
    "os.makedirs(os.path.dirname(SUBMISSION_SAVE_PATH), exist_ok=True)\n",
    "sub_df.to_csv(SUBMISSION_SAVE_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n제출 파일이 저장되었습니다: {SUBMISSION_SAVE_PATH}\")\n",
    "\n",
    "# 참고: \"Some weights ... 'score.weight' ...\" 경고는 분류 헤드가 새로 초기화됨을 의미하며 정상입니다(학습으로 최적화됨).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
