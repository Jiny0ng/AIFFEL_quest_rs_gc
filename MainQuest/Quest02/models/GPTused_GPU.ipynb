{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c180212-4463-403a-949e-0756cdbed5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "===== FOLD 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/tmp/ipykernel_724/1653869033.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 03:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.459217</td>\n",
       "      <td>0.862025</td>\n",
       "      <td>0.860784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>0.562917</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>0.877815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.780887</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>0.877647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[경고] classification_report 출력 중 예외: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 2/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_724/1653869033.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 03:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.394522</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>0.876206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.530662</td>\n",
       "      <td>0.889873</td>\n",
       "      <td>0.890191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.708104</td>\n",
       "      <td>0.894937</td>\n",
       "      <td>0.894686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[경고] classification_report 출력 중 예외: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 3/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_724/1653869033.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 03:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.369532</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.892060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.473257</td>\n",
       "      <td>0.889873</td>\n",
       "      <td>0.887987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.621584</td>\n",
       "      <td>0.889873</td>\n",
       "      <td>0.889185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[경고] classification_report 출력 중 예외: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 4/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_724/1653869033.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 03:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.465721</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.873552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.473327</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.886353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.894937</td>\n",
       "      <td>0.895452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[경고] classification_report 출력 중 예외: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 5/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_724/1653869033.py:216: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 03:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>0.524042</td>\n",
       "      <td>0.813924</td>\n",
       "      <td>0.816262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.448257</td>\n",
       "      <td>0.865823</td>\n",
       "      <td>0.865987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.492572</td>\n",
       "      <td>0.902532</td>\n",
       "      <td>0.902646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[경고] classification_report 출력 중 예외: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[안내] 템플릿 행수(3950) != test 샘플 수(500). test 기준 새 프레임 생성.\n",
      "\n",
      "제출 파일이 저장되었습니다: /home/jovyan/work/DL thon/data/GPT_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DKTC 5-클래스 분류 (GPT 기반, KoGPT2) - GPU 안정 학습 버전\n",
    "# - 학습: $HOME/work/DL thon/data/train.csv\n",
    "# - 예측: $HOME/work/DL thon/data/test.csv\n",
    "# - 제출 템플릿(읽기 전용): $HOME/work/DL thon/data/train.csv  ← 요청대로 템플릿 사용\n",
    "# - 결과 저장: $HOME/work/DL thon/data/GPT_submission.csv\n",
    "# =========================\n",
    "\n",
    "# 0) 필수 라이브러리 설치(없으면 설치)\n",
    "import sys, subprocess, importlib, os\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "for p in [\"transformers\", \"datasets\", \"accelerate\", \"scikit-learn\", \"pandas\", \"numpy\", \"torch\"]:\n",
    "    ensure(p)\n",
    "\n",
    "# 1) 임포트\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import inspect\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# EarlyStoppingCallback은 구버전에 없을 수 있음(있을 때만 사용)\n",
    "try:\n",
    "    from transformers import EarlyStoppingCallback\n",
    "    HAS_EARLY_STOP = True\n",
    "except Exception:\n",
    "    EarlyStoppingCallback = None\n",
    "    HAS_EARLY_STOP = False\n",
    "\n",
    "# 2) 고정 경로(요청사항 반영)\n",
    "TRAIN_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/train.csv\"\n",
    "TEST_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/test.csv\"\n",
    "SUBMISSION_TEMPLATE_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/train.csv\"     # 읽기 전용 템플릿(요청대로)\n",
    "SUBMISSION_SAVE_PATH = os.getenv(\"HOME\") + \"/work/DL thon/data/GPT_submission.csv\"  # 새 파일로 저장\n",
    "\n",
    "# 3) 기본 설정\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "MODEL_NAME = \"skt/kogpt2-base-v2\"  # 한국어 GPT-2 (Decoder-only)\n",
    "NUM_LABELS = 5\n",
    "NUM_FOLDS  = 5         # 데이터가 적으면 3 권장\n",
    "EPOCHS     = 3\n",
    "BATCH_SIZE = 8\n",
    "LR         = 2e-5\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# 4) CSV 안전 읽기(인코딩 자동 감지)\n",
    "def read_csv_smart(path: str) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\", \"ISO-8859-1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "# 5) 라벨 정규화 설정\n",
    "CANONICAL_LABELS = [\"협박\",\"갈취\",\"직장 내 괴롭힘\",\"기타 괴롭힘\",\"일상 대화\"]\n",
    "CODE_MAP = {\"협박\":\"00\",\"갈취\":\"01\",\"직장 내 괴롭힘\":\"02\",\"기타 괴롭힘\":\"03\",\"일상 대화\":\"04\"}\n",
    "ALIASES = {\n",
    "    \"협박\": {\"협박\",\"협박대화\",\"협박 대화\"},\n",
    "    \"갈취\": {\"갈취\",\"갈취대화\",\"갈취 대화\"},\n",
    "    \"직장 내 괴롭힘\": {\"직장내괴롭힘\",\"직장 내 괴롭힘\",\"직장 내괴롭힘\",\"직장내 괴롭힘\",\"직장 내 괴롭힘대화\",\"직장 내 괴롭힘 대화\",\"직장내괴롭힘대화\"},\n",
    "    \"기타 괴롭힘\": {\"기타 괴롭힘\",\"기타괴롭힘\",\"기타-괴롭힘\",\"기타 괴롭힘대화\",\"기타 괴롭힘 대화\"},\n",
    "    \"일상 대화\": {\"일상 대화\",\"일상대화\",\"일반\",\"일반 대화\",\"일반대화\"},\n",
    "}\n",
    "def normalize_label_name(x: str) -> str:\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"(대화|유형|클래스|라벨)$\", \"\", s).strip()\n",
    "    s_cmp = re.sub(r\"[\\s\\-\\_]\", \"\", s)\n",
    "    for canon, variants in ALIASES.items():\n",
    "        if s in variants or s_cmp in {re.sub(r'[\\s\\-\\_]', '', v) for v in variants}:\n",
    "            return canon\n",
    "    if \"협박\" in s: return \"협박\"\n",
    "    if \"갈취\" in s: return \"갈취\"\n",
    "    if (\"직장\" in s) and (\"괴롭힘\" in s): return \"직장 내 괴롭힘\"\n",
    "    if \"괴롭힘\" in s: return \"기타 괴롭힘\"\n",
    "    if (\"일상\" in s) or (\"일반\" in s): return \"일상 대화\"\n",
    "    if s in CANONICAL_LABELS: return s\n",
    "    return s\n",
    "\n",
    "# 6) 컬럼 자동 탐지\n",
    "def infer_text_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"text\",\"sentence\",\"utterance\",\"dialogue\",\"dialog\",\"conversation\",\"content\",\"data\",\"message\",\"문장\",\"대화\",\"텍스트\",\"내용\"]\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            sample = df[c].dropna().astype(str).head(20).tolist()\n",
    "            if any(len(str(t).strip()) > 0 for t in sample):\n",
    "                if c.lower() in candidates or c in candidates:\n",
    "                    return c\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if not obj_cols:\n",
    "        raise ValueError(\"텍스트 컬럼(object dtype)을 찾을 수 없습니다.\")\n",
    "    best_col = max(obj_cols, key=lambda c: df[c].dropna().astype(str).str.len().mean())\n",
    "    return best_col\n",
    "\n",
    "def infer_label_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"label\",\"labels\",\"class\",\"category\",\"target\",\"y\",\"라벨\",\"클래스\",\"카테고리\"]\n",
    "    for c in df.columns:\n",
    "        if c.lower() in candidates or c in candidates:\n",
    "            return c\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    for c in obj_cols:\n",
    "        uniq = set(df[c].dropna().astype(str).unique())\n",
    "        if any(normalize_label_name(u) in CANONICAL_LABELS for u in uniq):\n",
    "            return c\n",
    "    raise ValueError(\"라벨 컬럼을 자동 탐지할 수 없습니다. label/class 등으로 지정해 주세요.\")\n",
    "\n",
    "# 7) 데이터 로드 (원본 불변)\n",
    "train_df = read_csv_smart(TRAIN_PATH)\n",
    "test_df  = read_csv_smart(TEST_PATH)\n",
    "sub_tmpl = read_csv_smart(SUBMISSION_TEMPLATE_PATH)  # 요청대로 train.csv를 템플릿으로 사용\n",
    "\n",
    "# 8) 텍스트/라벨 컬럼 파악 및 정규화\n",
    "TEXT_COL = infer_text_column(train_df)\n",
    "LABEL_COL = infer_label_column(train_df)\n",
    "\n",
    "y_raw = train_df[LABEL_COL].astype(str).apply(normalize_label_name)\n",
    "unknown = sorted(set(y_raw.unique()) - set(CANONICAL_LABELS))\n",
    "if unknown:\n",
    "    raise ValueError(f\"다음 라벨 매핑 불가: {unknown} → normalize_label_name()에 패턴을 추가하세요.\")\n",
    "name2id = {name: i for i, name in enumerate(CANONICAL_LABELS)}\n",
    "id2name = {i: name for name, i in name2id.items()}\n",
    "y = y_raw.map(name2id)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "train_texts = train_df[TEXT_COL].astype(str).apply(clean_text).tolist()\n",
    "test_texts  = test_df[infer_text_column(test_df)].astype(str).apply(clean_text).tolist()\n",
    "\n",
    "# 8-1) FOLD 안전 설정\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "min_class_count = int(class_counts.min())\n",
    "if NUM_FOLDS > min_class_count:\n",
    "    old = NUM_FOLDS\n",
    "    NUM_FOLDS = max(2, min_class_count)\n",
    "    print(f\"[안내] 클래스 최소 개수({min_class_count})보다 폴드 수({old})가 큽니다 → {NUM_FOLDS}로 조정\")\n",
    "\n",
    "# 9) 토크나이저/모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    id2label={i: id2name[i] for i in range(NUM_LABELS)},\n",
    "    label2id={id2name[i]: i for i in range(NUM_LABELS)},\n",
    ")\n",
    "\n",
    "def tokenize_batch(texts: List[str]):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "# 10) 데이터셋\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "# 클래스 가중치 직접 계산(특정 fold에서 일부 클래스가 없어도 안전)\n",
    "def make_class_weight(labels: List[int], num_labels: int) -> torch.Tensor:\n",
    "    labels = np.array(labels, dtype=int)\n",
    "    N = len(labels); K = num_labels\n",
    "    counts = np.bincount(labels, minlength=K)\n",
    "    weights = np.zeros(K, dtype=np.float32)\n",
    "    for c in range(K):\n",
    "        weights[c] = N / (K * counts[c]) if counts[c] > 0 else 0.0\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# 11) Trainer 서브클래스(가중치 적용) - **kwargs로 추가 인자 흡수\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  # device 이동은 compute_loss에서\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 12) 평가 지표\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro}\n",
    "\n",
    "# == TrainingArguments를 버전에 맞춰 구성 (GPU 우선, 자동 안전 재시도 지원) ==\n",
    "def build_training_args(output_dir: str, fold_seed: int, fp16_try: bool = True) -> TrainingArguments:\n",
    "    params = set(inspect.signature(TrainingArguments.__init__).parameters.keys())\n",
    "    desired = dict(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        seed=fold_seed,\n",
    "        dataloader_num_workers=2,\n",
    "        fp16=bool(fp16_try and torch.cuda.is_available()),\n",
    "        bf16=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        report_to=[],\n",
    "        # GPU 사용: no_cuda/use_cpu는 지정하지 않음\n",
    "    )\n",
    "    kw = {k: v for k, v in desired.items() if k in params}\n",
    "    eval_key = \"evaluation_strategy\" if \"evaluation_strategy\" in params else (\"eval_strategy\" if \"eval_strategy\" in params else None)\n",
    "    save_ok = \"save_strategy\" in params\n",
    "    if \"load_best_model_at_end\" in kw and kw[\"load_best_model_at_end\"]:\n",
    "        if (eval_key is None) or (not save_ok):\n",
    "            kw[\"load_best_model_at_end\"] = False\n",
    "        else:\n",
    "            if eval_key not in kw:\n",
    "                kw[eval_key] = kw.get(\"save_strategy\", \"epoch\")\n",
    "            else:\n",
    "                if kw[eval_key] != kw.get(\"save_strategy\", None):\n",
    "                    kw[eval_key] = kw.get(\"save_strategy\", \"epoch\")\n",
    "    return TrainingArguments(**kw)\n",
    "\n",
    "# 13) Stratified K-Fold 분할 + 학습/예측 (GPU 시도 → 실패 시 안전 모드 재시도)\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "test_logits_accum = None\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train_texts, y), start=1):\n",
    "    print(f\"\\n===== FOLD {fold}/{NUM_FOLDS} =====\")\n",
    "    trn_texts = [train_texts[i] for i in trn_idx]\n",
    "    val_texts = [train_texts[i] for i in val_idx]\n",
    "    trn_labels = pd.Series(y).iloc[trn_idx].tolist()\n",
    "    val_labels = pd.Series(y).iloc[val_idx].tolist()\n",
    "\n",
    "    trn_enc = tokenize_batch(trn_texts)\n",
    "    val_enc = tokenize_batch(val_texts)\n",
    "    tst_enc = tokenize_batch(test_texts)\n",
    "\n",
    "    # 사전 검증: 라벨 범위\n",
    "    uniq_trn = sorted(set(trn_labels))\n",
    "    assert all(0 <= int(lb) < NUM_LABELS for lb in uniq_trn), f\"라벨이 0~{NUM_LABELS-1} 범위를 벗어났습니다: {uniq_trn}\"\n",
    "\n",
    "    # 데이터셋\n",
    "    train_ds = SimpleDataset(trn_enc, trn_labels)\n",
    "    val_ds   = SimpleDataset(val_enc, val_labels)\n",
    "    test_ds  = SimpleDataset(tst_enc, labels=None)\n",
    "\n",
    "    # 클래스 가중치\n",
    "    cls_weights = make_class_weight(trn_labels, NUM_LABELS)\n",
    "\n",
    "    # 모델\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.resize_token_embeddings(len(tokenizer))  # 임베딩 크기 동기화\n",
    "\n",
    "    # 토큰 id 범위 점검(훈련/검증/테스트 전체)\n",
    "    def max_token_id(enc):\n",
    "        return max(int(np.max(ids)) for ids in enc[\"input_ids\"])\n",
    "    max_ids = [max_token_id(trn_enc), max_token_id(val_enc), max_token_id(tst_enc)]\n",
    "    emb_n = model.get_input_embeddings().num_embeddings\n",
    "    assert max(max_ids) < emb_n, f\"토큰 id({max(max_ids)})가 임베딩 크기({emb_n})를 초과합니다.\"\n",
    "\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "    out_dir = f\"./gpt2_dktc_fold{fold}\"\n",
    "\n",
    "    # 1차: GPU + (가능하면) fp16로 시도\n",
    "    args = build_training_args(out_dir, SEED + fold, fp16_try=True)\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)] if HAS_EARLY_STOP else []\n",
    "\n",
    "    def make_trainer(train_args: TrainingArguments):\n",
    "        return WeightedTrainer(\n",
    "            model=model,\n",
    "            args=train_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            class_weights=cls_weights,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "    trainer = make_trainer(args)\n",
    "\n",
    "    def train_with_auto_retry(trainer_obj, first_args):\n",
    "        try:\n",
    "            return trainer_obj.train(), trainer_obj, first_args\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            if \"CUDA\" in msg or \"device-side assert\" in msg or \"cublas\" in msg or \"illegal memory access\" in msg:\n",
    "                warnings.warn(f\"[경고] CUDA 관련 예외 감지: {msg[:120]}... → 안전 모드 재시도(fp16=False)\")\n",
    "                # 메모리/상태 정리\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                # 안전 모드: fp16 끄고 동일 설정 재생성\n",
    "                safe_args = build_training_args(first_args.output_dir, first_args.seed, fp16_try=False)\n",
    "                safe_trainer = make_trainer(safe_args)\n",
    "                return safe_trainer.train(), safe_trainer, safe_args\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    _, trainer, used_args = train_with_auto_retry(trainer, args)\n",
    "\n",
    "    # 검증 성능 리포트(가능하면)\n",
    "    try:\n",
    "        val_pred_logits = trainer.predict(val_ds).predictions\n",
    "        val_preds = np.argmax(val_pred_logits, axis=-1)\n",
    "        print(classification_report(\n",
    "            val_labels, val_preds,\n",
    "            target_names=[id2name[i] for i in range(NUM_LABELS)],\n",
    "            digits=4\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"[경고] classification_report 출력 중 예외: {e}\")\n",
    "\n",
    "    # 테스트 로짓 앙상블(평균)\n",
    "    test_pred_logits = trainer.predict(test_ds).predictions  # [N_test, num_labels]\n",
    "    if test_logits_accum is None:\n",
    "        test_logits_accum = test_pred_logits\n",
    "    else:\n",
    "        test_logits_accum += test_pred_logits\n",
    "\n",
    "# 14) 테스트 최종 예측\n",
    "test_logits_mean = test_logits_accum / NUM_FOLDS\n",
    "test_pred_ids = np.argmax(test_logits_mean, axis=-1)  # 0~4\n",
    "id2name_local = {i: n for i, n in enumerate(CANONICAL_LABELS)}\n",
    "test_pred_names = [id2name_local[i] for i in test_pred_ids]\n",
    "test_pred_codes = [CODE_MAP[name] for name in test_pred_names]\n",
    "\n",
    "# 15) 제출 파일 생성(원본 템플릿은 읽기 전용, 새 파일에 저장)\n",
    "sub_df = sub_tmpl.copy()\n",
    "candidate_cols = [\"label\",\"class\",\"target\",\"pred\",\"prediction\"]\n",
    "label_col_to_use = None\n",
    "for c in sub_df.columns:\n",
    "    if c.lower() in candidate_cols:\n",
    "        label_col_to_use = c\n",
    "        break\n",
    "if label_col_to_use is None:\n",
    "    label_col_to_use = \"label\"\n",
    "    if \"label\" not in sub_df.columns:\n",
    "        sub_df[\"label\"] = None\n",
    "\n",
    "# 템플릿과 test 길이 불일치 시 test 기준으로 생성\n",
    "if len(sub_df) != len(test_pred_codes):\n",
    "    print(f\"[안내] 템플릿 행수({len(sub_df)}) != test 샘플 수({len(test_pred_codes)}). test 기준 새 프레임 생성.\")\n",
    "    sub_df = pd.DataFrame({label_col_to_use: test_pred_codes})\n",
    "else:\n",
    "    sub_df[label_col_to_use] = test_pred_codes\n",
    "\n",
    "# 저장(원본 파일 미변경)\n",
    "os.makedirs(os.path.dirname(SUBMISSION_SAVE_PATH), exist_ok=True)\n",
    "sub_df.to_csv(SUBMISSION_SAVE_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n제출 파일이 저장되었습니다: {SUBMISSION_SAVE_PATH}\")\n",
    "\n",
    "# 참고: 'Some weights ... score.weight' 경고는 분류 헤드 초기화 알림으로 정상입니다(학습으로 최적화됩니다).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c713c-2277-49ad-8eb4-1cddc1b2b7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
