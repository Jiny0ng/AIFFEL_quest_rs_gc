{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0ff898-5a37-4082-b189-599f84dd49f7",
   "metadata": {},
   "source": [
    "#### 뉴스 카테고리 다중분류 프로젝트\n",
    "\n",
    "##### 목표: Vocabulary Size 변화가 뉴스 카테고리 다중분류 모델의 성능에 어떤 영향을 주는지 탐구\n",
    "\n",
    "##### 데이터셋: Reuters 뉴스 분류 데이터셋\n",
    "\n",
    "##### 사용 모델: 나이브 베이즈(MultinomialNB, ComplementNB), 로지스틱 회귀, 서포트 벡터 머신(SVM), 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅(앙상블), 순환 신경망(RNN)\n",
    "\n",
    "##### 평가 지표: Accuracy, F1 Score (macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afb89e5-1076-4f5a-8d49-5d104bc985da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 03:52:20.493259: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-01 03:52:20.565818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-01 03:52:22.330309: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/opt/conda/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 302: Error loading CUDA libraries. GPU will not be used. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Vocab Size: 2000 ==========\n",
      "[ML] MultinomialNB: Acc=0.6910, F1=0.1655, Time=0.02s\n",
      "[ML] ComplementNB: Acc=0.7569, F1=0.4000, Time=0.02s\n",
      "[ML] Logistic: Acc=0.7974, F1=0.4647, Time=7.00s\n",
      "[ML] SVM: Acc=0.8321, F1=0.6733, Time=2.48s\n",
      "[ML] DecisionTree: Acc=0.6972, F1=0.4533, Time=5.46s\n",
      "[ML] RandomForest: Acc=0.7787, F1=0.4861, Time=24.76s\n",
      "[ML] GradientBoosting: Acc=0.7618, F1=0.4974, Time=1092.25s\n",
      "[ML] Voting: Acc=0.7974, F1=0.6009, Time=1097.24s\n",
      "[RNN] Vocab=2000, Epoch=1, Acc=0.4924, F1=0.0316\n",
      "[RNN] Vocab=2000, Epoch=2, Acc=0.4982, F1=0.0286\n",
      "[RNN] Vocab=2000, Epoch=3, Acc=0.5490, F1=0.0489\n",
      "[RNN] Vocab=2000, Epoch=4, Acc=0.5859, F1=0.0619\n",
      "[RNN] Vocab=2000, Epoch=5, Acc=0.6020, F1=0.0723\n",
      "[RNN] Vocab=2000, Epoch=6, Acc=0.6291, F1=0.1145\n",
      "[RNN] Vocab=2000, Epoch=7, Acc=0.6371, F1=0.1331\n",
      "[RNN] Vocab=2000, Epoch=8, Acc=0.6073, F1=0.1423\n",
      "[RNN] Vocab=2000, Epoch=9, Acc=0.6701, F1=0.1747\n",
      "[RNN] Vocab=2000, Epoch=10, Acc=0.6647, F1=0.1760\n",
      "[RNN] Vocab=2000, Epoch=11, Acc=0.6888, F1=0.2115\n",
      "[RNN] Vocab=2000, Epoch=12, Acc=0.6892, F1=0.2179\n",
      "[RNN] Vocab=2000, Epoch=13, Acc=0.6955, F1=0.2137\n",
      "[RNN] Vocab=2000, Epoch=14, Acc=0.6999, F1=0.2398\n",
      "[RNN] Vocab=2000, Epoch=15, Acc=0.7035, F1=0.2493\n",
      "[RNN] Vocab=2000, Epoch=16, Acc=0.6968, F1=0.2481\n",
      "[RNN] Vocab=2000, Epoch=17, Acc=0.7115, F1=0.2651\n",
      "[RNN] Vocab=2000, Epoch=18, Acc=0.7004, F1=0.2964\n",
      "[RNN] Vocab=2000, Epoch=19, Acc=0.7057, F1=0.3164\n",
      "[RNN] Vocab=2000, Epoch=20, Acc=0.6986, F1=0.3319\n",
      "[RNN] Vocab=2000: Final Acc=0.6986, F1=0.3319, Time=568.80s\n",
      "\n",
      "========== Vocab Size: 5000 ==========\n",
      "[ML] MultinomialNB: Acc=0.6732, F1=0.1102, Time=0.03s\n",
      "[ML] ComplementNB: Acc=0.7707, F1=0.4820, Time=0.03s\n",
      "[ML] Logistic: Acc=0.7979, F1=0.4814, Time=8.96s\n",
      "[ML] SVM: Acc=0.8290, F1=0.6813, Time=2.78s\n",
      "[ML] DecisionTree: Acc=0.6963, F1=0.4499, Time=6.79s\n",
      "[ML] RandomForest: Acc=0.7671, F1=0.4514, Time=33.25s\n",
      "[ML] GradientBoosting: Acc=0.7752, F1=0.5911, Time=1234.07s\n",
      "[ML] Voting: Acc=0.8161, F1=0.6695, Time=1232.94s\n",
      "[RNN] Vocab=5000, Epoch=1, Acc=0.4987, F1=0.0291\n",
      "[RNN] Vocab=5000, Epoch=2, Acc=0.5178, F1=0.0466\n",
      "[RNN] Vocab=5000, Epoch=3, Acc=0.5463, F1=0.0583\n",
      "[RNN] Vocab=5000, Epoch=4, Acc=0.5886, F1=0.0744\n",
      "[RNN] Vocab=5000, Epoch=5, Acc=0.5855, F1=0.0968\n",
      "[RNN] Vocab=5000, Epoch=6, Acc=0.6131, F1=0.1111\n",
      "[RNN] Vocab=5000, Epoch=7, Acc=0.6104, F1=0.1187\n",
      "[RNN] Vocab=5000, Epoch=8, Acc=0.6358, F1=0.1332\n",
      "[RNN] Vocab=5000, Epoch=9, Acc=0.6211, F1=0.1422\n",
      "[RNN] Vocab=5000, Epoch=10, Acc=0.6309, F1=0.1458\n",
      "[RNN] Vocab=5000, Epoch=11, Acc=0.6465, F1=0.1501\n",
      "[RNN] Vocab=5000, Epoch=12, Acc=0.6140, F1=0.1505\n",
      "[RNN] Vocab=5000, Epoch=13, Acc=0.6621, F1=0.1673\n",
      "[RNN] Vocab=5000, Epoch=14, Acc=0.6581, F1=0.1728\n",
      "[RNN] Vocab=5000, Epoch=15, Acc=0.6625, F1=0.1769\n",
      "[RNN] Vocab=5000, Epoch=16, Acc=0.6665, F1=0.1846\n",
      "[RNN] Vocab=5000, Epoch=17, Acc=0.6656, F1=0.1911\n",
      "[RNN] Vocab=5000, Epoch=18, Acc=0.6647, F1=0.2412\n",
      "[RNN] Vocab=5000, Epoch=19, Acc=0.6759, F1=0.2461\n",
      "[RNN] Vocab=5000, Epoch=20, Acc=0.6812, F1=0.2497\n",
      "[RNN] Vocab=5000: Final Acc=0.6812, F1=0.2497, Time=608.55s\n",
      "\n",
      "========== Vocab Size: 10000 ==========\n",
      "[ML] MultinomialNB: Acc=0.6572, F1=0.0969, Time=0.03s\n",
      "[ML] ComplementNB: Acc=0.7707, F1=0.4784, Time=0.03s\n",
      "[ML] Logistic: Acc=0.7956, F1=0.4721, Time=12.15s\n",
      "[ML] SVM: Acc=0.8299, F1=0.6808, Time=1.23s\n",
      "[ML] DecisionTree: Acc=0.6941, F1=0.4622, Time=7.33s\n",
      "[ML] RandomForest: Acc=0.7533, F1=0.4305, Time=45.39s\n",
      "[ML] GradientBoosting: Acc=0.7698, F1=0.5637, Time=1304.01s\n",
      "[ML] Voting: Acc=0.8126, F1=0.6446, Time=1307.72s\n",
      "[RNN] Vocab=10000, Epoch=1, Acc=0.4889, F1=0.0285\n",
      "[RNN] Vocab=10000, Epoch=2, Acc=0.5183, F1=0.0443\n",
      "[RNN] Vocab=10000, Epoch=3, Acc=0.5735, F1=0.0571\n",
      "[RNN] Vocab=10000, Epoch=4, Acc=0.5988, F1=0.0719\n",
      "[RNN] Vocab=10000, Epoch=5, Acc=0.5975, F1=0.0835\n",
      "[RNN] Vocab=10000, Epoch=6, Acc=0.6109, F1=0.1182\n",
      "[RNN] Vocab=10000, Epoch=7, Acc=0.6211, F1=0.1231\n",
      "[RNN] Vocab=10000, Epoch=8, Acc=0.6256, F1=0.1460\n",
      "[RNN] Vocab=10000, Epoch=9, Acc=0.6505, F1=0.1563\n",
      "[RNN] Vocab=10000, Epoch=10, Acc=0.6603, F1=0.1887\n",
      "[RNN] Vocab=10000, Epoch=11, Acc=0.6692, F1=0.1923\n",
      "[RNN] Vocab=10000, Epoch=12, Acc=0.6625, F1=0.1987\n",
      "[RNN] Vocab=10000, Epoch=13, Acc=0.6603, F1=0.2116\n",
      "[RNN] Vocab=10000, Epoch=14, Acc=0.6656, F1=0.2412\n",
      "[RNN] Vocab=10000, Epoch=15, Acc=0.6634, F1=0.2551\n",
      "[RNN] Vocab=10000, Epoch=16, Acc=0.6719, F1=0.2462\n",
      "[RNN] Vocab=10000, Epoch=17, Acc=0.6830, F1=0.2786\n",
      "[RNN] Vocab=10000, Epoch=18, Acc=0.6750, F1=0.2786\n",
      "[RNN] Vocab=10000, Epoch=19, Acc=0.6665, F1=0.2789\n",
      "[RNN] Vocab=10000, Epoch=20, Acc=0.6772, F1=0.2968\n",
      "[RNN] Vocab=10000: Final Acc=0.6772, F1=0.2968, Time=616.13s\n",
      "\n",
      "========== Vocab Size: 20000 ==========\n",
      "[ML] MultinomialNB: Acc=0.6193, F1=0.0789, Time=0.05s\n",
      "[ML] ComplementNB: Acc=0.7671, F1=0.4632, Time=0.07s\n",
      "[ML] Logistic: Acc=0.7921, F1=0.4582, Time=16.78s\n",
      "[ML] SVM: Acc=0.8290, F1=0.6892, Time=1.21s\n",
      "[ML] DecisionTree: Acc=0.6981, F1=0.4655, Time=8.03s\n",
      "[ML] RandomForest: Acc=0.7435, F1=0.4107, Time=67.12s\n",
      "[ML] GradientBoosting: Acc=0.7636, F1=0.5645, Time=1375.05s\n",
      "[ML] Voting: Acc=0.8175, F1=0.6629, Time=1380.09s\n",
      "[RNN] Vocab=20000, Epoch=1, Acc=0.4987, F1=0.0288\n",
      "[RNN] Vocab=20000, Epoch=2, Acc=0.4911, F1=0.0297\n",
      "[RNN] Vocab=20000, Epoch=3, Acc=0.4706, F1=0.0307\n",
      "[RNN] Vocab=20000, Epoch=4, Acc=0.5365, F1=0.0514\n",
      "[RNN] Vocab=20000, Epoch=5, Acc=0.5824, F1=0.0654\n",
      "[RNN] Vocab=20000, Epoch=6, Acc=0.6109, F1=0.0872\n",
      "[RNN] Vocab=20000, Epoch=7, Acc=0.6443, F1=0.1027\n",
      "[RNN] Vocab=20000, Epoch=8, Acc=0.6647, F1=0.1460\n",
      "[RNN] Vocab=20000, Epoch=9, Acc=0.6643, F1=0.1699\n",
      "[RNN] Vocab=20000, Epoch=10, Acc=0.6781, F1=0.1924\n",
      "[RNN] Vocab=20000, Epoch=11, Acc=0.6848, F1=0.2123\n",
      "[RNN] Vocab=20000, Epoch=12, Acc=0.6883, F1=0.2284\n",
      "[RNN] Vocab=20000, Epoch=13, Acc=0.6879, F1=0.2441\n",
      "[RNN] Vocab=20000, Epoch=14, Acc=0.6923, F1=0.2490\n",
      "[RNN] Vocab=20000, Epoch=15, Acc=0.7053, F1=0.2767\n",
      "[RNN] Vocab=20000, Epoch=16, Acc=0.6990, F1=0.3203\n",
      "[RNN] Vocab=20000, Epoch=17, Acc=0.6968, F1=0.3315\n",
      "[RNN] Vocab=20000, Epoch=18, Acc=0.7070, F1=0.3379\n",
      "[RNN] Vocab=20000, Epoch=19, Acc=0.6977, F1=0.3368\n",
      "[RNN] Vocab=20000, Epoch=20, Acc=0.6995, F1=0.3485\n",
      "[RNN] Vocab=20000: Final Acc=0.6995, F1=0.3485, Time=612.87s\n",
      "\n",
      "===== Final Results =====\n",
      "    Vocab             Model  Accuracy        F1      Time(s)\n",
      "0    2000     MultinomialNB  0.691006  0.165467     0.024340\n",
      "1    2000      ComplementNB  0.756901  0.400023     0.021042\n",
      "2    2000          Logistic  0.797418  0.464684     6.996656\n",
      "3    2000               SVM  0.832146  0.673261     2.479149\n",
      "4    2000      DecisionTree  0.697240  0.453309     5.457391\n",
      "5    2000      RandomForest  0.778718  0.486102    24.761083\n",
      "6    2000  GradientBoosting  0.761799  0.497375  1092.249696\n",
      "7    2000            Voting  0.797418  0.600886  1097.235519\n",
      "8    2000               RNN  0.698575  0.331947   568.803532\n",
      "9    5000     MultinomialNB  0.673197  0.110173     0.029021\n",
      "10   5000      ComplementNB  0.770703  0.482035     0.026889\n",
      "11   5000          Logistic  0.797863  0.481380     8.962677\n",
      "12   5000               SVM  0.829029  0.681329     2.776831\n",
      "13   5000      DecisionTree  0.696349  0.449869     6.792757\n",
      "14   5000      RandomForest  0.767142  0.451447    33.251271\n",
      "15   5000  GradientBoosting  0.775156  0.591104  1234.066155\n",
      "16   5000            Voting  0.816118  0.669508  1232.942070\n",
      "17   5000               RNN  0.681211  0.249682   608.546757\n",
      "18  10000     MultinomialNB  0.657168  0.096870     0.034062\n",
      "19  10000      ComplementNB  0.770703  0.478358     0.032167\n",
      "20  10000          Logistic  0.795637  0.472114    12.149510\n",
      "21  10000               SVM  0.829920  0.680843     1.229376\n",
      "22  10000      DecisionTree  0.694123  0.462224     7.334864\n",
      "23  10000      RandomForest  0.753339  0.430536    45.391037\n",
      "24  10000  GradientBoosting  0.769813  0.563680  1304.012024\n",
      "25  10000            Voting  0.812556  0.644568  1307.724428\n",
      "26  10000               RNN  0.677204  0.296807   616.130151\n",
      "27  20000     MultinomialNB  0.619323  0.078869     0.048600\n",
      "28  20000      ComplementNB  0.767142  0.463235     0.065513\n",
      "29  20000          Logistic  0.792075  0.458198    16.783816\n",
      "30  20000               SVM  0.829029  0.689207     1.213929\n",
      "31  20000      DecisionTree  0.698130  0.465509     8.030038\n",
      "32  20000      RandomForest  0.743544  0.410654    67.123129\n",
      "33  20000  GradientBoosting  0.763580  0.564469  1375.047456\n",
      "34  20000            Voting  0.817453  0.662856  1380.092913\n",
      "35  20000               RNN  0.699466  0.348509   612.866618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# =========================================\n",
    "# 1. 하이퍼파라미터\n",
    "# =========================================\n",
    "vocab_sizes = [2000, 5000, 10000, 20000]\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20   # Early Stopping 적용하므로 넉넉하게\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================================\n",
    "# 2. 전통 ML 모델 정의 (+ Voting 추가)\n",
    "# =========================================\n",
    "ml_models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"Logistic\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": LinearSVC(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"Voting\": VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"lr\", LogisticRegression(C=10000, penalty=\"l2\", max_iter=1000)),\n",
    "            (\"cb\", ComplementNB()),\n",
    "            (\"grbt\", GradientBoostingClassifier(random_state=0))\n",
    "        ],\n",
    "        voting=\"soft\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# 3. RNN 모델 정의 (PyTorch)\n",
    "# =========================================\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_classes=46):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h, _) = self.rnn(x)\n",
    "        out = self.fc(h[-1])\n",
    "        return out\n",
    "\n",
    "# =========================================\n",
    "# 4. 결과 저장 리스트\n",
    "# =========================================\n",
    "results = []\n",
    "\n",
    "# =========================================\n",
    "# 5. 메인 루프\n",
    "# =========================================\n",
    "for vocab in vocab_sizes:\n",
    "    print(f\"\\n========== Vocab Size: {vocab} ==========\")\n",
    "\n",
    "    # ----- 데이터 로드 -----\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=vocab, test_split=0.2)\n",
    "\n",
    "    # ----- 텍스트 복원 (ML 전용) -----\n",
    "    word_index = reuters.get_word_index()\n",
    "    index_word = {v+3: k for k, v in word_index.items()}\n",
    "    index_word[0], index_word[1], index_word[2] = \"<PAD>\", \"<START>\", \"<UNK>\"\n",
    "\n",
    "    x_train_text = [\" \".join([index_word.get(i, \"?\") for i in seq]) for seq in x_train]\n",
    "    x_test_text  = [\" \".join([index_word.get(i, \"?\") for i in seq]) for seq in x_test]\n",
    "\n",
    "    # ----- TF-IDF (ML 전용) -----\n",
    "    vectorizer = TfidfVectorizer(max_features=vocab)\n",
    "    X_train = vectorizer.fit_transform(x_train_text)\n",
    "    X_test  = vectorizer.transform(x_test_text)\n",
    "\n",
    "    # ----- 전통 ML 모델 학습/평가 -----\n",
    "    for name, model in ml_models.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        f1  = f1_score(y_test, preds, average=\"macro\")\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        results.append((vocab, name, acc, f1, elapsed))\n",
    "        print(f\"[ML] {name}: Acc={acc:.4f}, F1={f1:.4f}, Time={elapsed:.2f}s\")\n",
    "\n",
    "    # ----- RNN 데이터 준비 -----\n",
    "    x_train_pad = pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "    x_test_pad = pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "    x_train_tensor = torch.tensor(x_train_pad, dtype=torch.long)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    x_test_tensor = torch.tensor(x_test_pad, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(x_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=BATCH_SIZE)\n",
    "\n",
    "    # ----- RNN 모델 정의 -----\n",
    "    model = RNNClassifier(vocab_size=vocab, num_classes=46).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # ----- 학습 + Early Stopping -----\n",
    "    best_f1 = 0\n",
    "    patience, wait = 2, 0   # f1 2번 감소하면 stop\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # --- Validation (여기서는 test set 사용) ---\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                pred_labels = preds.argmax(dim=1)\n",
    "                all_preds.extend(pred_labels.cpu().numpy())\n",
    "                all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "        print(f\"[RNN] Vocab={vocab}, Epoch={epoch+1}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "        # --- Early Stopping 체크 ---\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    results.append((vocab, \"RNN\", acc, f1, elapsed))\n",
    "    print(f\"[RNN] Vocab={vocab}: Final Acc={acc:.4f}, F1={f1:.4f}, Time={elapsed:.2f}s\")\n",
    "\n",
    "# =========================================\n",
    "# 6. 결과 DataFrame\n",
    "# =========================================\n",
    "df = pd.DataFrame(results, columns=[\"Vocab\", \"Model\", \"Accuracy\", \"F1\", \"Time(s)\"])\n",
    "print(\"\\n===== Final Results =====\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff882b-35f7-4de8-8923-2c5a2b661c05",
   "metadata": {},
   "source": [
    "## 모델별 Vocab Size 영향 해석\n",
    "\n",
    "### 1. **MultinomialNB**\n",
    "\n",
    "* Vocab이 작을 때는 자주 등장하는 단어 중심으로 안정적인 성능을 보임\n",
    "* Vocab이 커질수록 저빈도 단어까지 개별 토큰으로 포함되어 희소성이 증가\n",
    "* 결과적으로 학습 데이터에서 충분히 보지 못한 단어들이 많아져 **성능이 점차 저하**됨\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **ComplementNB**\n",
    "\n",
    "* 희소 데이터에 강하도록 설계된 모델\n",
    "* Vocab이 커지더라도 저빈도 단어의 영향을 상대적으로 잘 보완\n",
    "* 따라서 **vocab 크기에 크게 흔들리지 않고 안정적인 성능 유지**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Logistic Regression**\n",
    "\n",
    "* Vocab이 커질수록 더 많은 단어 정보를 활용할 수 있어 분류 경계가 정교해짐\n",
    "* 전체적으로 성능이 꾸준히 안정적이나, SVM보다는 낮은 수준\n",
    "* 학습 시간은 비교적 길지만 Voting이나 GBM보다는 효율적\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **SVM**\n",
    "\n",
    "* 가장 뚜렷하게 vocab 증가 효과를 본 모델\n",
    "* Vocab이 커질수록 텍스트 표현력이 향상되어 분류 경계가 더 정확해짐\n",
    "* 전 구간에서 안정적으로 **가장 높은 F1 스코어와 정확도 기록**\n",
    "* 학습 시간도 1\\~3초 수준으로 매우 효율적\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Decision Tree**\n",
    "\n",
    "* Vocab이 커져도 단어의 희소성이 커지면 분할 기준을 잘 잡기 어려움\n",
    "* 그 결과 성능 개선은 거의 없고, 일부 구간에서는 성능이 들쭉날쭉\n",
    "* 단일 트리의 한계로 인해 안정적인 분류에는 제약\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Random Forest**\n",
    "\n",
    "* 다수의 트리를 합쳐서 성능은 Decision Tree보다는 안정적\n",
    "* 그러나 vocab이 커질수록 희소 단어의 영향이 누적되어 성능 개선이 제한적\n",
    "* 학습 시간은 20\\~60초 이상 소요되며, 성능 대비 효율성이 낮음\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Gradient Boosting**\n",
    "\n",
    "* Vocab 증가에 따라 분류 경계가 조금 더 정교해져 성능이 소폭 개선\n",
    "* 하지만 학습 시간이 매우 길어 (20분 이상) 실용성은 떨어짐\n",
    "* 성능과 시간을 함께 고려했을 때 비효율적인 모델\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Voting (앙상블)**\n",
    "\n",
    "* Logistic, ComplementNB, GradientBoosting을 결합\n",
    "* Vocab이 커질수록 안정적으로 성능이 향상 (F1 ≈ 0.60 → 0.66)\n",
    "* 그러나 GradientBoosting이 포함되어 있어 학습 시간이 매우 길어짐\n",
    "* 안정적인 성능은 보장하지만 **시간 비용이 큰 모델**\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **RNN**\n",
    "\n",
    "* Vocab이 커질수록 임베딩 파라미터 수가 급격히 증가\n",
    "* 데이터 크기(약 1만 문서)가 충분하지 않아 저빈도 단어를 학습하기 어려움\n",
    "* 결과적으로 성능 향상이 제한적이고 일부 구간에서는 오히려 성능이 흔들림\n",
    "* Accuracy는 0.70 수준까지 도달했지만, F1 스코어는 여전히 낮음 (0.25\\~0.35)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc9b3e-2172-43bd-adab-88b9b1a6a6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
